{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in goal_shot_rows.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import data_prep\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "model_path = '0427Model_2M.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_data_from_csv(path_to_csv):\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    # get list of ilocs of rows where the 'Event' column is 'GOAL or SHOT or MISS'\n",
    "    goal_shot_rows = df.loc[df['Event'].isin(['GOAL', 'SHOT', 'MISS'])]\n",
    "\n",
    "    # shift the dataframe by 1 row\n",
    "    shifted_df = df.shift(1)\n",
    "    shifted_df.columns = ['prev_' + name for name in df.columns]\n",
    "\n",
    "    # combine goal_shot_rows with shifted_df, but using rows from goal_shot_rows\n",
    "    goal_shot_rows = pd.concat([goal_shot_rows, shifted_df.loc[goal_shot_rows.index]], axis=1)\n",
    "\n",
    "    goal_shot_rows = goal_shot_rows[goal_shot_rows['Period'] != 0]\n",
    "    goal_shot_rows = goal_shot_rows[goal_shot_rows['Period'] != 5]\n",
    "\n",
    "    # Create \"home_or_away\" column based on if \"Ev_team\" is the same as \"Home_Team\" or \"Away_Team\"\n",
    "    goal_shot_rows['home_or_away'] = goal_shot_rows.apply(lambda x: 'home' if x['Ev_Team'] == x['Home_Team'] else 'away', axis=1)\n",
    "    \n",
    "    # check to see if on power play if \"home_or_away\" is equal to \"home\" and \"Strength\" is equal to \"5x4\" or \"home_or_away\" is equal to \"away\" and \"Strength\" is equal to \"4x5\"\n",
    "    goal_shot_rows['true_strength'] = goal_shot_rows.apply(lambda x: \"5x4\" if (x['home_or_away'] == 'home' and x['Strength'] == '5x4' ) or (x['home_or_away'] == 'away' and x['Strength'] == '4x5') else \"5x5\", axis=1)\n",
    "\n",
    "    # similar as above, but for 5x3\n",
    "    goal_shot_rows['true_strength'] = goal_shot_rows.apply(lambda x: \"5x3\" if (x['home_or_away'] == 'home' and x['Strength'] == '5x3' ) or (x['home_or_away'] == 'away' and x['Strength'] == '3x5') else x['true_strength'], axis=1)\n",
    "\n",
    "    # similar as above, but for 6x5\n",
    "    goal_shot_rows['true_strength'] = goal_shot_rows.apply(lambda x: \"6x5\" if (x['home_or_away'] == 'home' and x['Strength'] == '6x5' ) or (x['home_or_away'] == 'away' and x['Strength'] == '5x6') else x['true_strength'], axis=1)\n",
    "\n",
    "    # similar as above, but for 4x3\n",
    "    goal_shot_rows['true_strength'] = goal_shot_rows.apply(lambda x: \"4x3\" if (x['home_or_away'] == 'home' and x['Strength'] == '4x3' ) or (x['home_or_away'] == 'away' and x['Strength'] == '3x4') else x['true_strength'], axis=1)\n",
    "\n",
    "    # similar as above,but for 3x3\n",
    "    goal_shot_rows['true_strength'] = goal_shot_rows.apply(lambda x: \"3x3\" if (x['home_or_away'] == 'home' and x['Strength'] == '3x3' ) or (x['home_or_away'] == 'away' and x['Strength'] == '3x3') else x['true_strength'], axis=1)\n",
    "\n",
    "    # check to see if a shot is on an empty net by checking if home_or_away is equal to \"home\" and \"Away_Goalie\" is equal to \"None\" or home_or_away is equal to \"away\" and \"Home_Goalie\" is equal to \"None\"\n",
    "    goal_shot_rows['on_empty_net'] = goal_shot_rows.apply(lambda x: 1 if (x['home_or_away'] == 'home' and x['Away_Goalie'] == '') or (x['home_or_away'] == 'away' and x['Home_Goalie'] == '') else 0, axis=1)\n",
    "\n",
    "\n",
    "    val_input_data = goal_shot_rows[['Event', 'Period', 'Seconds_Elapsed', 'Strength', 'Type', 'xC', 'yC', 'home_or_away', 'true_strength', 'on_empty_net', 'prev_Event', 'prev_Period', 'prev_Seconds_Elapsed', 'prev_Strength', 'prev_Type', 'prev_xC', 'prev_yC']]\n",
    "    val_result_data = goal_shot_rows['Event']\n",
    "    val_result_data = val_result_data.apply(lambda x: 1 if x == \"GOAL\" else 0)\n",
    "\n",
    "    val_input_data = val_input_data.drop(columns=['Event'])\n",
    "\n",
    "    # Split out the data between numeric values (can carry forward) and categorical values (need to be turned into binary columns)\n",
    "    val_input_data_numeric = val_input_data[['Period', 'Seconds_Elapsed', 'xC', 'yC', 'on_empty_net','prev_Seconds_Elapsed', 'prev_xC', 'prev_yC']]\n",
    "    val_input_data_categorical = pd.DataFrame()\n",
    "\n",
    "    for column in ['Type', 'prev_Event', 'true_strength', 'home_or_away']:\n",
    "        dummy_columns = pd.get_dummies(val_input_data[column])\n",
    "        val_input_data_categorical = pd.concat([dummy_columns, val_input_data_categorical], axis=1)\n",
    "\n",
    "    for column in ['prev_Event', 'prev_Strength', 'prev_Type']:\n",
    "        dummy_columns = pd.get_dummies(val_input_data[column])\n",
    "\n",
    "        # rename all columns with a prefix of \"prev_\"\n",
    "        dummy_columns.columns = ['prev_' + str(col) for col in dummy_columns.columns]\n",
    "        val_input_data_categorical = pd.concat([val_input_data_categorical, dummy_columns], axis=1)\n",
    "        \n",
    "    # change all True/False to 1/0\n",
    "    val_input_data_categorical = val_input_data_categorical.applymap(lambda x: 1 if x == True else 0)\n",
    "\n",
    "    # combine the two dataframes\n",
    "    val_input_data_combined = pd.concat([val_input_data_numeric, val_input_data_categorical], axis=1)\n",
    "\n",
    "    #set val_input_data2 to be all floats\n",
    "    val_input_data_combined = val_input_data_combined.astype(float)\n",
    "\n",
    "    columns = ['Period', 'Seconds_Elapsed', 'xC', 'yC', 'prev_Seconds_Elapsed',\n",
    "            'prev_xC', 'prev_yC', 'BLOCK', 'CHL', 'DELPEN', 'FAC', 'GIVE', 'HIT',\n",
    "            'MISS', 'PENL', 'SHOT', 'STOP', 'TAKE', 'BACKHAND', 'DEFLECTED',\n",
    "            'SLAP SHOT', 'SNAP SHOT', 'TIP-IN', 'WRAP-AROUND', 'WRIST SHOT', '0x0',\n",
    "            '3x3', '3x4', '3x5', '4x3', '4x4', '4x5', '5x3', '5x4', '5x5', '6x5',\n",
    "            'on_empty_net', 'away', 'home', '5v4', '5v5',\n",
    "            'prev_BLOCK', 'prev_CHL', 'prev_DELPEN', 'prev_FAC', 'prev_GIVE',\n",
    "            'prev_HIT', 'prev_MISS', 'prev_PENL', 'prev_SHOT', 'prev_STOP',\n",
    "            'prev_TAKE', 'prev_0x5', 'prev_3x3', 'prev_3x4', 'prev_3x5', 'prev_4x3',\n",
    "            'prev_4x4', 'prev_4x5', 'prev_5x3', 'prev_5x4', 'prev_5x5', 'prev_5x6', 'prev_6x5',\n",
    "            'prev_BACKHAND', 'prev_DEFLECTED',\n",
    "            'prev_PS-Covering puck in crease(0 min)',\n",
    "            'prev_PS-Goalkeeper displaced net(0 min)',\n",
    "            'prev_PS-Holding on breakaway(0 min)',\n",
    "            'prev_PS-Hooking on breakaway(0 min)',\n",
    "            'prev_PS-Slash on breakaway(0 min)',\n",
    "            'prev_PS-Throw object at puck(0 min)',\n",
    "            'prev_PS-Tripping on breakaway(0 min)', 'prev_SLAP SHOT',\n",
    "            'prev_SNAP SHOT', 'prev_TIP-IN', 'prev_WRAP-AROUND', 'prev_WRIST SHOT']\n",
    "\n",
    "    # For each column in input_data_combined, check if it exists in val_input_data_combined, if not add it as all 0s\n",
    "    for column in columns:\n",
    "        if column not in val_input_data_combined.columns:\n",
    "                val_input_data_combined[column] = 0\n",
    "\n",
    "    for column in val_input_data_combined.columns:\n",
    "        if column not in columns:\n",
    "                if column in val_input_data_combined.columns:\n",
    "                    val_input_data_combined = val_input_data_combined.drop(column, axis=1)\n",
    "\n",
    "\n",
    "    # reduce val_input_data_combined to have the same columns as input_data_combined\n",
    "    val_input_data_combined = val_input_data_combined[columns]\n",
    "\n",
    "    # change all NaN numbers to 0\n",
    "    val_input_data_combined = val_input_data_combined.fillna(0)\n",
    "\n",
    "    print(val_input_data_combined.shape)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(78, 240),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(240, 60),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(60, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    x = torch.tensor(val_input_data_combined.values, dtype=torch.float32)\n",
    "    y = torch.tensor(val_result_data.values, dtype=torch.float32)\n",
    "    result = model(x)\n",
    "\n",
    "    # add the probability of a goal to val_input_data_combined\n",
    "    goal_shot_rows['Goal_Probability'] = result.detach().numpy()\n",
    "\n",
    "    return val_input_data_combined, val_result_data, goal_shot_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Evaluation\n",
    "\n",
    "For game prediction, I'm stealing a bit from MoneyPuck, the inputs are:\n",
    "\n",
    "1. Home team xG ratrio full for season w/linear delay\n",
    "2. same, but for away team\n",
    "3. home team days of rest\n",
    "4. away team days of rest\n",
    "5. num of games by the home team\n",
    "6. num of games by the away team\n",
    "\n",
    "\n",
    "Eventually, need to add some amount of goalie calculation/generic player strength calculation, but that will have to wait.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_game_xG_totals(original_input, game_id, state = \"any\"):\n",
    "    game_rows = original_input[original_input[\"Game_Id\"] == game_id]\n",
    "\n",
    "    if state != \"any\":\n",
    "        game_rows = game_rows[game_rows[\"Strength\"] == state]\n",
    "\n",
    "    # get first value of Home_Team and Away_Team\n",
    "    home_team = game_rows[\"Home_Team\"].iloc[0]\n",
    "    away_team = game_rows[\"Away_Team\"].iloc[0]\n",
    "\n",
    "    # get all rows where the Ev_Team is the home team\n",
    "    home_team_rows = game_rows[game_rows[\"Ev_Team\"] == home_team]\n",
    "\n",
    "    # get all rows where the Ev_Team is the away team\n",
    "    away_team_rows = game_rows[game_rows[\"Ev_Team\"] == away_team]\n",
    "\n",
    "    # add a new column \"total_xG\" to home_team_rows and away_team_rows\n",
    "    home_team_rows[\"total_xG\"] = home_team_rows[\"Goal_Probability\"].cumsum()\n",
    "    away_team_rows[\"total_xG\"] = away_team_rows[\"Goal_Probability\"].cumsum()\n",
    "\n",
    "    # time elapsed is (period - 1) * 1200 + seconds_elapsed\n",
    "    home_team_rows[\"Seconds_Elapsed\"] = (home_team_rows[\"Period\"] - 1) * 1200 + home_team_rows[\"Seconds_Elapsed\"]\n",
    "    home_team_rows[\"Minutes_Elapsed\"] = home_team_rows[\"Seconds_Elapsed\"] / 60\n",
    "\n",
    "    # print home_team_rows to csv\n",
    "    # home_team_rows.to_csv(\"home_team_rows.csv\")\n",
    "\n",
    "    # time elapsed is (period - 1) * 1200 + seconds_elapsed\n",
    "    away_team_rows[\"Seconds_Elapsed\"] = (away_team_rows[\"Period\"] - 1) * 1200 + away_team_rows[\"Seconds_Elapsed\"]\n",
    "    away_team_rows[\"Minutes_Elapsed\"] = away_team_rows[\"Seconds_Elapsed\"] / 60\n",
    "\n",
    "    # away_team_rows.to_csv(\"away_team_rows.csv\")\n",
    "\n",
    "    total_home_xG = home_team_rows[\"total_xG\"].iloc[-1].round(2)\n",
    "    total_away_xG = away_team_rows[\"total_xG\"].iloc[-1].round(2)\n",
    "\n",
    "    # combine home_team_rows and away_team_rows into one dataframe\n",
    "    combined_rows = pd.concat([home_team_rows, away_team_rows])\n",
    "\n",
    "    home_score = home_team_rows[\"Home_Score\"].iloc[-1]\n",
    "    away_score = away_team_rows[\"Away_Score\"].iloc[-1]\n",
    "\n",
    "    return combined_rows, home_team, total_home_xG, away_team, total_away_xG, home_score, away_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This has to be done season by season, or the game_ids will merge together in a broken way\n",
    "def get_season_xG_weights(original_input, season_id):\n",
    "    # split original_input into multiple dataframes based off of game_id\n",
    "    game_ids = original_input[\"Game_Id\"].unique()\n",
    "    # remove all instances of game_ids that start with 3\n",
    "    game_ids = [x for x in game_ids if str(x)[0] != \"3\"]\n",
    "\n",
    "    nhl_teams = {}\n",
    "    nhl_teams_predicted_xG = {}\n",
    "\n",
    "    results = []\n",
    "    predictions = []\n",
    "    predictions_rounded = []\n",
    "\n",
    "    training_input = []\n",
    "\n",
    "    # read in the schedule csv into a dictionary, using game_id as the key\n",
    "    schedule = pd.read_csv(\"schedule/2020-2023_schedule.csv\")\n",
    "    schedule = schedule.set_index(\"game_id\")\n",
    "\n",
    "    for game_id in game_ids:\n",
    "        # print(f\"Game ID: {game_id}\")\n",
    "        combined_rows, home_team, total_home_xG, away_team, total_away_xG, home_score, away_score = get_single_game_xG_totals(original_input, game_id, \"any\")\n",
    "\n",
    "        # add the total_xG of the home team and away team to nhl_teams\n",
    "        if home_team not in nhl_teams:\n",
    "            nhl_teams[home_team] = {\"xG_percent\": [], \"total_games\": 0, \"game_dates\" : []}\n",
    "        if away_team not in nhl_teams:\n",
    "            nhl_teams[away_team] = {\"xG_percent\": [], \"total_games\": 0, \"game_dates\" : []}\n",
    "\n",
    "        # if both teams have more than 20 games played, create a prediciton for the game\n",
    "        if nhl_teams[home_team][\"total_games\"] >= 20 and nhl_teams[away_team][\"total_games\"] >= 20:\n",
    "            # get the average of last 20 games for home_team and away_team\n",
    "            # home_xG_percent_avg = np.mean(nhl_teams[home_team][\"xG_percent\"])\n",
    "            # away_xG_percent_avg = np.mean(nhl_teams[away_team][\"xG_percent\"])\n",
    "\n",
    "            # home_xG_percent_avg = home_xG_percent_avg / (home_xG_percent_avg + away_xG_percent_avg)\n",
    "            # away_xG_percent_avg = away_xG_percent_avg / (home_xG_percent_avg + away_xG_percent_avg)\n",
    "\n",
    "            # # Apply linear decay to each home_team's xg_percent game by game\n",
    "            decay_rate = 0.01  # Adjust the decay rate as desired\n",
    "\n",
    "\n",
    "            # I think this logic still has some flaws, but we'll get to it later\n",
    "\n",
    "            # Calculate the decayed xg_percent for each game\n",
    "            home_decayed_xg_percent = np.zeros_like(nhl_teams[home_team][\"xG_percent\"])\n",
    "            len_home_games = len(nhl_teams[home_team][\"xG_percent\"])\n",
    "            for i in range(len_home_games):\n",
    "                home_decayed_xg_percent[i] = nhl_teams[home_team][\"xG_percent\"][(len_home_games - i -1 )] * (1 - (decay_rate * i))\n",
    "\n",
    "            away_decayed_xg_percent = np.zeros_like(nhl_teams[away_team][\"xG_percent\"])\n",
    "            len_away_games = len(nhl_teams[away_team][\"xG_percent\"])\n",
    "            for i in range(len_away_games):\n",
    "                away_decayed_xg_percent[i] = nhl_teams[away_team][\"xG_percent\"][(len_away_games - i - 1)] * (1 - (decay_rate * i))\n",
    "\n",
    "            home_xG_percent_sum = np.sum(home_decayed_xg_percent)\n",
    "            away_xG_percent_sum = np.sum(away_decayed_xg_percent)\n",
    "            \n",
    "            home_xG_percent_avg = home_xG_percent_sum / (home_xG_percent_sum + away_xG_percent_sum)\n",
    "            away_xG_percent_avg = away_xG_percent_sum / (home_xG_percent_sum + away_xG_percent_sum)\n",
    "\n",
    "           \n",
    "\n",
    "            # Calculate the days of rest for each team (format is YYYY-MM-DD):\n",
    "            home_team_prev_date = nhl_teams[home_team][\"game_dates\"][-1]\n",
    "            away_team_prev_date = nhl_teams[away_team][\"game_dates\"][-1]\n",
    "\n",
    "\n",
    "            # Todo: add a way to get the season id programatically off of the game's date\n",
    "            alt_game_id = season_id + \"0\" + str(game_id)\n",
    "            # print(alt_game_id)\n",
    "\n",
    "            schedule_row = schedule.loc[int(alt_game_id)]\n",
    "            home_team_score = schedule_row[\"home_score\"]\n",
    "            away_team_score = schedule_row[\"away_score\"]\n",
    "\n",
    "            # Calculate the days of rest for each team\n",
    "            # calucluate the difference in days between two strings with format YYYY-MM-DD\n",
    "            # Convert strings to datetime objects\n",
    "            home_prev_date = datetime.strptime(home_team_prev_date, \"%Y-%m-%d\")\n",
    "            away_prev_date = datetime.strptime(away_team_prev_date, \"%Y-%m-%d\")\n",
    "            current_date = datetime.strptime(combined_rows[\"Date\"].iloc[0], \"%Y-%m-%d\")\n",
    "\n",
    "            home_team_days_of_rest = (current_date - home_prev_date).days - 1\n",
    "            away_team_days_of_rest = (current_date - away_prev_date).days - 1\n",
    "\n",
    "            if (home_team_days_of_rest > 3):\n",
    "                home_team_days_of_rest = 3\n",
    "            if (away_team_days_of_rest > 3):\n",
    "                away_team_days_of_rest = 3\n",
    "\n",
    "            if home_team not in nhl_teams_predicted_xG:\n",
    "                nhl_teams_predicted_xG[home_team] = {}\n",
    "            if away_team not in nhl_teams_predicted_xG:\n",
    "                nhl_teams_predicted_xG[away_team] = {}\n",
    "            nhl_teams_predicted_xG[home_team][game_id] = {\"xg_sum\": home_xG_percent_sum, \"days_rest\": home_team_days_of_rest, \"num_games\": nhl_teams[home_team][\"total_games\"]}\n",
    "            nhl_teams_predicted_xG[away_team][game_id] = {\"xg_sum\": away_xG_percent_sum, \"days_rest\": away_team_days_of_rest, \"num_games\": nhl_teams[away_team][\"total_games\"]}\n",
    "            training_input.append({\"home_xg_sum\": home_xG_percent_sum, \"away_xg_sum\": away_xG_percent_sum,\n",
    "                                    \"home_days_rest\": home_team_days_of_rest, \"away_days_rest\": away_team_days_of_rest,\n",
    "                                    \"home_num_games\": nhl_teams[home_team][\"total_games\"],\n",
    "                                    \"away_num_games\": nhl_teams[away_team][\"total_games\"],\n",
    "                                    \"game_result\": home_team_score > away_team_score})\n",
    "            \n",
    "\n",
    "            results.append(home_team_score > away_team_score)\n",
    "            predictions.append(home_xG_percent_avg)\n",
    "            predictions_rounded.append(home_xG_percent_avg.round())\n",
    "\n",
    "        home_xG_percent = total_home_xG / (total_home_xG + total_away_xG)\n",
    "        away_xG_percent = total_away_xG / (total_home_xG + total_away_xG)\n",
    "\n",
    "        nhl_teams[home_team][\"xG_percent\"].append(home_xG_percent)\n",
    "        nhl_teams[away_team][\"xG_percent\"].append(away_xG_percent)\n",
    "        nhl_teams[home_team][\"total_games\"] += 1\n",
    "        nhl_teams[away_team][\"total_games\"] += 1\n",
    "        nhl_teams[home_team][\"game_dates\"].append(combined_rows[\"Date\"].iloc[0])\n",
    "        nhl_teams[away_team][\"game_dates\"].append(combined_rows[\"Date\"].iloc[0])\n",
    "\n",
    "    # get the accuracy of the model\n",
    "    results_tensor = torch.tensor(results, dtype=torch.float32)\n",
    "    predictions_tensor = torch.tensor(predictions, dtype=torch.float32)\n",
    "    predictions_rounded_tensor = torch.tensor(predictions_rounded, dtype=torch.float32)\n",
    "\n",
    "    accuracy = (predictions_rounded_tensor == results_tensor).float().mean()\n",
    "\n",
    "    # calculate log loss of predicitons\n",
    "    log_loss = nn.BCELoss()\n",
    "    log_loss_results = log_loss(predictions_tensor, results_tensor)\n",
    "\n",
    "    # print(f\"Accuracy {accuracy}\")\n",
    "    # print(f\"log loss {log_loss_results}\")\n",
    "\n",
    "    return nhl_teams, nhl_teams_predicted_xG, training_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating input data for the game prediciton neural network\n",
    "\n",
    "We need:\n",
    "\n",
    "1. To get result data (preferably through the schedule, but working on it)\n",
    "2. To get the input data:\n",
    "    1. Home team xG percentage sum for season w/linear delay\n",
    "    2. same, but for away team\n",
    "    3. home team days of rest\n",
    "    4. away team days of rest\n",
    "    5. num of games by the home team\n",
    "    6. num of games by the away team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoshG\\AppData\\Local\\Temp\\ipykernel_37696\\2609486208.py:61: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  val_input_data_categorical = val_input_data_categorical.applymap(lambda x: 1 if x == True else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121992, 78)\n"
     ]
    }
   ],
   "source": [
    "# read in nhl_pbp_20232024.csv\n",
    "val_input_data, val_result_data, original_input = create_input_data_from_csv(r\"C:\\Users\\JoshG\\hockey_scraper_data\\csvs\\nhl_pbp_20222023.csv\")\n",
    "\n",
    "nhl_teams, nhl_teams_predicted_xG, training_input = get_season_xG_weights(original_input, \"2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoshG\\AppData\\Local\\Temp\\ipykernel_37696\\2609486208.py:61: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  val_input_data_categorical = val_input_data_categorical.applymap(lambda x: 1 if x == True else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78581, 78)\n"
     ]
    }
   ],
   "source": [
    "# read in nhl_pbp_20232024.csv\n",
    "val_input_data2, val_result_data2, original_input2 = create_input_data_from_csv(r\"C:\\Users\\JoshG\\hockey_scraper_data\\csvs\\nhl_pbp_20202021.csv\")\n",
    "\n",
    "nhl_teams2, nhl_teams_predicted_xG2, training_input2 = get_season_xG_weights(original_input2, \"2020\")\n",
    "\n",
    "# combine the two training_inputs\n",
    "training_input = training_input + training_input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['home_xg_sum', 'away_xg_sum', 'home_days_rest', 'away_days_rest',\n",
      "       'home_num_games', 'away_num_games', 'game_result'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convert training_input to a dataframe\n",
    "training_input_df = pd.DataFrame(training_input)\n",
    "\n",
    "print(training_input_df.columns)\n",
    "\n",
    "game_result_data_df = training_input_df[\"game_result\"]\n",
    "\n",
    "training_input_df = training_input_df.drop(columns=['game_result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_days_rest\n",
      "1    972\n",
      "2    238\n",
      "0    206\n",
      "3    113\n",
      "Name: count, dtype: int64\n",
      "away_days_rest\n",
      "1    910\n",
      "0    336\n",
      "2    208\n",
      "3     75\n",
      "Name: count, dtype: int64\n",
      "(1529, 6)\n"
     ]
    }
   ],
   "source": [
    "print(training_input_df[\"home_days_rest\"].value_counts())\n",
    "print(training_input_df[\"away_days_rest\"].value_counts())\n",
    "print(training_input_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "x = torch.tensor(training_input_df.values, dtype=torch.float32)\n",
    "y = torch.tensor(game_result_data_df.values, dtype=torch.float32)\n",
    "\n",
    "x.to(device)\n",
    "y.to(device)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "loss_fn.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.6637235283851624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     adam(\n\u001b[0;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m    168\u001b[0m         grads,\n\u001b[0;32m    169\u001b[0m         exp_avgs,\n\u001b[0;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    172\u001b[0m         state_steps,\n\u001b[0;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m func(params,\n\u001b[0;32m    317\u001b[0m      grads,\n\u001b[0;32m    318\u001b[0m      exp_avgs,\n\u001b[0;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    321\u001b[0m      state_steps,\n\u001b[0;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:508\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 508\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 500000\n",
    "batch_size = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        Xbatch = x[i:i+batch_size]\n",
    "        y_pred = model(Xbatch.cuda())\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        #loss = loss_fn(y_pred, ybatch)\n",
    "        loss = loss_fn(y_pred.squeeze(), ybatch.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'{epoch}, {loss}')\n",
    "\n",
    "\n",
    "# compute accuracy (no_grad is optional)\n",
    "# y_pred = model(x.cuda()).cuda()\n",
    " \n",
    "\n",
    "torch.save(model.state_dict(), 'Game_prediciton_model_0427_500k.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([979])) that is different to the input size (torch.Size([979, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     10\u001b[0m log_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m---> 11\u001b[0m log_loss_results \u001b[38;5;241m=\u001b[39m log_loss(y_pred, y)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# convert the tensor to a numpy array\u001b[39;00m\n\u001b[0;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32mc:\\Users\\JoshG\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3118\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3116\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3121\u001b[0m     )\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([979])) that is different to the input size (torch.Size([979, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Game_prediciton_model_0427_25k.pt'))\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "x = torch.tensor(training_input_df.values, dtype=torch.float32)\n",
    "y = torch.tensor(game_result_data_df.values, dtype=torch.float32)\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "\n",
    "\n",
    "# convert the tensor to a numpy array\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y = y.detach().numpy()\n",
    "\n",
    "\n",
    "# accuracy = (y_pred.round() == y).float().mean()\\\n",
    "accuracy_results = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    # print(f\"Prediction: {y_pred[i][0]} Actual: {y[i]}\")\n",
    "    if y_pred[i][0].round() == y[i]:\n",
    "        accuracy_results.append(1)\n",
    "    else:\n",
    "        accuracy_results.append(0)\n",
    "\n",
    "accuracy = sum(accuracy_results) / len(accuracy_results)\n",
    "print(f\"Accuracy {accuracy}\")\n",
    "print(f\"log loss {log_loss_results}\")\n",
    "# calculate log loss of predicitons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JoshG\\AppData\\Local\\Temp\\ipykernel_37696\\2609486208.py:61: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  val_input_data_categorical = val_input_data_categorical.applymap(lambda x: 1 if x == True else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121546, 78)\n"
     ]
    }
   ],
   "source": [
    "val_input_data, val_result_data, original_input = create_input_data_from_csv(r\"C:\\Users\\JoshG\\hockey_scraper_data\\csvs\\nhl_pbp_20212022.csv\")\n",
    "\n",
    "nhl_teams, nhl_teams_predicted_xG, training_input = get_season_xG_weights(original_input, \"2021\")\n",
    "\n",
    "# Convert training_input to a dataframe\n",
    "training_input_df = pd.DataFrame(training_input)\n",
    "game_result_data_df = training_input_df[\"game_result\"]\n",
    "training_input_df = training_input_df.drop(columns=['game_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5720122574055159\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Game_prediciton_model_0427_25k.pt'))\n",
    "model.eval()\n",
    "model.cpu()\n",
    "\n",
    "x = torch.tensor(training_input_df.values, dtype=torch.float32)\n",
    "y = torch.tensor(game_result_data_df.values, dtype=torch.float32)\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "\n",
    "# convert the tensor to a numpy array\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y = y.detach().numpy()\n",
    "\n",
    "\n",
    "# accuracy = (y_pred.round() == y).float().mean()\\\n",
    "accuracy_results = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    # print(f\"Prediction: {y_pred[i][0]} Actual: {y[i]}\")\n",
    "    if y_pred[i][0].round() == y[i]:\n",
    "        accuracy_results.append(1)\n",
    "    else:\n",
    "        accuracy_results.append(0)\n",
    "\n",
    "accuracy = sum(accuracy_results) / len(accuracy_results)\n",
    "print(f\"Accuracy {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5, 0.4651162922382355, 43\n",
      "0.52, 0.5555555820465088, 54\n",
      "0.54, 0.6000000238418579, 40\n",
      "0.56, 0.5, 60\n",
      "0.58, 0.6000000238418579, 60\n",
      "0.6, 0.5384615659713745, 117\n",
      "0.62, 0.5957446694374084, 47\n",
      "0.64, 0.4864864945411682, 37\n",
      "0.66, 0.5909090638160706, 44\n",
      "0.68, 0.6666666865348816, 39\n",
      "0.7, 0.38461539149284363, 39\n",
      "0.72, 0.5, 38\n",
      "0.74, 0.7419354915618896, 31\n",
      "0.76, 0.5862069129943848, 29\n",
      "0.78, 0.6818181872367859, 22\n",
      "0.8, 0.692307710647583, 26\n",
      "0.82, 0.6875, 32\n",
      "0.84, 0.5652173757553101, 23\n",
      "0.86, 0.7222222089767456, 18\n",
      "0.88, 0.6470588445663452, 17\n",
      "0.9, 0.5714285969734192, 14\n",
      "0.92, 0.5833333134651184, 12\n",
      "0.94, 0.6666666865348816, 12\n",
      "0.96, 0.6428571343421936, 14\n",
      "0.98, 0.5495495200157166, 111\n"
     ]
    }
   ],
   "source": [
    "results = y\n",
    "predictions = y_pred\n",
    "results2 = results.flatten()\n",
    "predictions2 = predictions.flatten()\n",
    "\n",
    "\n",
    "for i in range(len(predictions2)):\n",
    "    if predictions2[i] < 0.5:\n",
    "        predictions2[i] = 1 - predictions2[i]\n",
    "        if results2[i] == False:\n",
    "            results2[i] = True\n",
    "        else:\n",
    "            results2[i] = False\n",
    "\n",
    "\n",
    "results_tensor = torch.tensor(results2)\n",
    "predictions_tensor = torch.tensor(predictions2)\n",
    "predictions_rounded = [int(x) for x in predictions2]\n",
    "predictions_rounded_tensor = torch.tensor(predictions_rounded)\n",
    "\n",
    "\n",
    "\n",
    "percentages = [x / 100.0 for x in range(50, 100, 2)]\n",
    "\n",
    "\n",
    "\n",
    "for i in percentages:\n",
    "\n",
    "    indices = torch.where((predictions_tensor >= i) & (predictions_tensor <= i + 0.02))\n",
    "    filtered_predictions_tensor = predictions_tensor[indices]\n",
    "    filtered_results_tensor = results_tensor[indices]\n",
    "    filtered_predictions_rounded_tensor = predictions_rounded_tensor[indices]\n",
    "\n",
    "    # convert filtered_results_tensor from 1/0 to True/False\n",
    "    filtered_results_tensor = filtered_results_tensor.bool()\n",
    "\n",
    "    accuracy_tensor = (filtered_results_tensor).float().mean()\n",
    "    # get length of filtered_results_tensor\n",
    "    length = len(filtered_results_tensor)\n",
    "    \n",
    "\n",
    "    print(f\"{i}, {accuracy_tensor}, {len(filtered_results_tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
